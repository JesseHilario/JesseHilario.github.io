---
title: "Case Study BellaBeat"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("devtools")
devtools::install_github("PolMine/RcppCWB")
devtools::install_github("tidyverse")
install.packages("janitor")
install.packages("lubridate")
```


## Scenario

You are a junior data analyst working on the marketing analyst team at Bellabeat, a high-tech manufacturer of health-focused products for women. Bellabeat is a successful small company, but they have the potential to become a larger player in the global smart device market. Urška Sršen, cofounder and Chief Creative Officer of Bellabeat, believes that analyzing smart device fitness data could help unlock new growth opportunities for the company. You have been asked to focus on one of Bellabeat’s products and analyze smart device data to gain insight into how consumers are using their smart devices. The insights you discover will then help guide marketing strategy for the company. You will present your analysis to the Bellabeat executive team along with your high-level recommendations for Bellabeat’s marketing strategy.


## Ask

#### Business Task

Analyze non-Bellabeat smart device usage data in order to gain insight into how the the Leaf tracker, Bellabeat's classic wellness tracker, might be used by consumers. Our insights can drive business decisions by adapting marketing strategy to target the consumers who use non-Bellabeat smart devices and how they specifically use their devices.


#### Guiding Questions

1. What are some trends in smart device usage?
2. How could these trends apply to Bellabeat customers?
3. How could these trends help influence Bellabeat marketing strategy?


## Prepare


#### Loading Packages

```{r packages, results='hide'}
library(tidyverse)
library(janitor)
library(lubridate)
```


#### About the dataset

The product of focus will be Leaf, Bellabeat’s classic wellness tracker that can be worn as a bracelet, necklace, or clip. The Leaf tracker connects to the Bellabeat app to track activity, sleep, and stress. We will use FitBit Data gathered from [FitBit Fitness Tracker Data](https://www.kaggle.com/datasets/arashnic/fitbit) (CC0: Public Domain, dataset made available through Mobius). This dataset was generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016-05.12.2016. 

This Kaggle data set contains personal fitness tracker from thirty fitbit users. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring. We will apply trends found with this dataset to Leaf marketing strategy.


#### Data storage, organization, and verification

Each dataset was stored as a Microsoft Excel CSV file. Almost all of the data could be classified as long; participants were repeated for every time that was recorded, allowing each participant to have multiple rows. Participants were anonymized with unique ID numbers.

We decided to use the following tables:

* **Daily Activity** 33 participants for 31 days tracking daily steps, distance, minutes, steps, and calories
* **Hourly Calories** 33 participants for 31 days tracking hourly calories burned
* **Hourly Intensities** 33 participants for 31 days tracking hourly total and average intensity
* **Daily Sleep** 24 participants for 31 days tracking daily minutes asleep and in bed
* **Hourly Steps** 33 participants for 31 days tracking hourly step totals


#### Importing dataset

```{r import dataset}
daily_activity <- read_csv("Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv")
day_sleep <- read_csv("sleepDay_merged.csv")

hour_calories <- read_csv("Fitabase Data 4.12.16-5.12.16/hourlyCalories_merged.csv")
hour_intensities <- read_csv("Fitabase Data 4.12.16-5.12.16/hourlyIntensities_merged.csv")
hour_steps <- read_csv("Fitabase Data 4.12.16-5.12.16/hourlySteps_merged.csv")
```

#### Data integrity and limitations

There were a few limitations that may threaten the credibility and integrity of the data. The data appears both reliable and original. The data is comprehensive in its measurement of different time specifications (from the day all the way to the second); however it is missing demographic data, has the bare minimum in sample size, and only lasts for two months. This may introduce sampling bias as the data 1) may not be representative of the population and 2) does not account for difference in trends outside of April and May. The data is also not current, ending almost 7 years from the time of this case study (2016). 

Although certain datasets was desirable for analysis, they were excluded as their validity was questionable. For instance, the WeightLogInfo dataset only included 8 participants; most only had one or two responses, making tracking progress impossible.

We can progress with caution to process our data.




## Process


#### Checking datasets

First we took a look at the datasets and verified the number of distinct participants.

```{r preview datasets}
glimpse(daily_activity)
n_distinct(daily_activity$Id)

glimpse(day_sleep)
n_distinct(day_sleep$Id)

glimpse(hour_calories)
n_distinct(hour_calories$Id)

glimpse(hour_intensities$Id)
n_distinct(hour_intensities$Id)

glimpse(hour_steps$Id)
n_distinct(hour_steps$Id)
```

#### Cleaning data

First we used the clean_names() function to make the column names more uniform and the colnames() function to check the changes.

```{r clean names}
daily_activity <- clean_names(daily_activity)
colnames(daily_activity)

day_sleep <- clean_names(day_sleep)
colnames(day_sleep)

hour_calories <- clean_names(hour_calories)
colnames(hour_calories)

hour_intensities <- clean_names(hour_intensities)
colnames(hour_intensities)

hour_steps <- clean_names(hour_steps)
colnames(hour_steps)
```

We also found that the variables id and date were not in the right data format, so we fixed them accordingly. We turned id into a string since each id represented individual people and not numerical integers.

```{r}
daily_activity <- daily_activity %>% 
  mutate(activity_date = mdy(daily_activity$activity_date),
         id = as.character(id))


```

Then, we looked if the data had any duplicated rows.

```{r}
sum(duplicated(daily_activity))
sum(duplicated())
sum(duplicated())
sum(duplicated())
sum(duplicated())
```

Guiding questions
● What tools are you choosing and why?
● Have you ensured your data’s integrity?
● What steps have you taken to ensure that your data is clean?
● How can you verify that your data is clean and ready to analyze?
● Have you documented your cleaning process so you can review and share those results?
Key tasks
1. Check the data for errors.
2. Choose your tools.
3. Transform the data so you can work with it effectively.
4. Document the cleaning process.
Deliverable
Documentation of any cleaning or manipulation of data


Finally, we merged the datasets for hourly 

```{r pressure, echo=FALSE}
nrow(daily_activity)
nrow(day_sleep)


nrow(hour_calories)
nrow(hour_intensities)
nrow(hour_steps)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
